{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980dab8b-e60c-486e-adaf-7d999f5bcb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import nibabel as nib\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pytorch_toolbelt import losses as L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c46d99-cffb-47b0-a9a1-c65f68f27f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea0359-3a1a-42a5-89dc-25d466c61047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_mask1,train_mask2,train_mask3,train_mask_all\n",
    "class train_dl(torch.utils.data.Dataset):\n",
    "    def __init__(self, frame):\n",
    "        self.frame =frame\n",
    "        self.image_files = [f for f in frame['train_images'].tolist()]\n",
    "        self.mask_files1 = [f for f in frame['train_mask1'].tolist()]\n",
    "        self.mask_files2 = [f for f in frame['train_mask2'].tolist()]\n",
    "        self.mask_files3 = [f for f in frame['train_mask3'].tolist()]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,mask=img_trf_train(str(self.image_files[index]),str(self.mask_files1[index]),str(self.mask_files2[index]),str(self.mask_files3[index]))\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "        \n",
    "class val_dl(torch.utils.data.Dataset):\n",
    "    def __init__(self,frame):\n",
    "        self.frame =frame\n",
    "        self.image_files = [f for f in frame['train_images'].tolist()]\n",
    "        self.mask_files1 = [f for f in frame['train_mask1'].tolist()]\n",
    "        self.mask_files2 = [f for f in frame['train_mask2'].tolist()]\n",
    "        self.mask_files3 = [f for f in frame['train_mask3'].tolist()]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,mask=img_trf_val(str(self.image_files[index]),str(self.mask_files1[index]),str(self.mask_files2[index]),str(self.mask_files3[index]))\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "\n",
    "\n",
    "class test_dl(torch.utils.data.Dataset):\n",
    "    def __init__(self,frame):\n",
    "        self.frame =frame\n",
    "        self.image_files = [f for f in frame['test_images'].tolist()]\n",
    "        self.mask_files1 = [f for f in frame['test_mask1'].tolist()]\n",
    "        self.mask_files2 = [f for f in frame['test_mask2'].tolist()]\n",
    "        self.mask_files3 = [f for f in frame['test_mask3'].tolist()]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,mask=img_trf_val(str(self.image_files[index]),str(self.mask_files1[index]),str(self.mask_files2[index]),str(self.mask_files3[index]))\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90c604-9beb-4352-b9f4-348170a7f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_trf_train(img_path,mask_path1,mask_path2,mask_path3):\n",
    "\n",
    "    img=cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "    msk1=cv2.imread(str(mask_path1), cv2.IMREAD_GRAYSCALE)\n",
    "    msk2=cv2.imread(str(mask_path2), cv2.IMREAD_GRAYSCALE)\n",
    "    msk3=cv2.imread(str(mask_path3), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    #edged = cv2.Canny(msk, 50, 200) #for edge\n",
    "    #msk=(edged > 0).astype(int) #for edge\n",
    "    max_side1=max(img.shape[0],img.shape[1])\n",
    "    max_side2=max(msk1.shape[0],msk1.shape[1])\n",
    "    max_side=max(max_side1,max_side2)\n",
    "    aug = A.Compose([\n",
    "        A.PadIfNeeded(min_height=max_side, min_width=max_side,border_mode=cv2.BORDER_CONSTANT,value=0,p=1),\n",
    "        A.Affine(scale=(0.8,1.1), shear=(-20,20), rotate=(-20,20), p=0.8,fit_output=True,keep_ratio=True,mode=0,cval=0),\n",
    "        #A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0, p=0.1),\n",
    "        #A.RandomBrightnessContrast(p=0.5),\n",
    "        A.Resize(128,128),\n",
    "    ],is_check_shapes=False)\n",
    "    \n",
    "    masks=np.stack([msk1,msk2,msk3],axis=0).T\n",
    "    augmented = aug(image=img, mask=masks)\n",
    "    \n",
    "    image_padded = torch.tensor(augmented['image'].T)\n",
    "    mask_padded = torch.tensor(augmented['mask'].T)\n",
    "    return image_padded, mask_padded\n",
    "\n",
    "\n",
    "def img_trf_val(img_path,mask_path1,mask_path2,mask_path3):\n",
    "\n",
    "    img=cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "    msk1=cv2.imread(str(mask_path1), cv2.IMREAD_GRAYSCALE)\n",
    "    msk2=cv2.imread(str(mask_path2), cv2.IMREAD_GRAYSCALE)\n",
    "    msk3=cv2.imread(str(mask_path3), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    #edged = cv2.Canny(msk, 50, 200) #for edge\n",
    "    #msk=(edged > 0).astype(int) #for edge\n",
    "    max_side1=max(img.shape[0],img.shape[1])\n",
    "    max_side2=max(msk1.shape[0],msk1.shape[1])\n",
    "    max_side=max(max_side1,max_side2)\n",
    "    aug = A.Compose([\n",
    "        A.PadIfNeeded(min_height=max_side, min_width=max_side,border_mode=cv2.BORDER_CONSTANT,value=0,p=1),\n",
    "        A.Resize(128,128),\n",
    "    ],is_check_shapes=False)\n",
    "    \n",
    "    masks=np.stack([msk1,msk2,msk3],axis=0).T\n",
    "\n",
    "    augmented = aug(image=img, mask=masks)\n",
    "    \n",
    "    image_padded = torch.tensor(augmented['image'].T)\n",
    "    mask_padded = torch.tensor(augmented['mask'].T)\n",
    "    return image_padded, mask_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d747ce-910f-4ab4-b980-b286fbb41538",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('train_data.csv')\n",
    "test_data=pd.read_csv('test_data.csv')\n",
    "train_df, val_df = train_test_split(train_data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47abcce8-e0b9-4dc4-857e-ec5e67a613d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dl(test_data)\n",
    "\n",
    "test_dataloader=DataLoader(test_dataset, batch_size=1,pin_memory=True,shuffle=False,num_workers=2)\n",
    "#test_features,test_labels = next(iter(test_dataloader))\n",
    "\n",
    "#test_set=torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_features,train_labels), batch_size=4,pin_memory=True,shuffle=True,num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7ffe9-09f6-4e16-a44b-b6a8ae8e22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=3,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "\n",
    "#preprocess_input = get_preprocessing_fn('resnet34', pretrained='imagenet')\n",
    "loss_function=L.DiceLoss(mode='multilabel')#binary vagy multiclass\n",
    "lr=1e-3\n",
    "optimizer=optim.Adam(model.parameters(), lr=lr, weight_decay=0.000001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.3, patience=3, verbose=True)\n",
    "model_name='./Unet_Res34_Imgnet_Dice_all_mask.pt'\n",
    "model.load_state_dict(torch.load(model_name, weights_only=True, map_location=torch.device('cuda')))\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c0413e-faf1-4008-8ad7-8f53dc35a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "model.eval()\n",
    "i=0\n",
    "for images, labels in test_dataloader:\n",
    "    images=images.unsqueeze(1)\n",
    "    images=images.to(device=device, dtype=torch.float) \n",
    "    outputs = model(images)\n",
    "    outputs=outputs.squeeze(0)\n",
    "    print(outputs.shape)\n",
    "    #save_image(outputs, 'o.png') save full batch\n",
    "    cv2.imwrite(f'output/unet/{i}.png', outputs.cpu().detach().numpy().T)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff9d4f-0d66-47e5-900a-cda334aeaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "\n",
    "model= smp.MAnet(\n",
    "    encoder_depth=4,                  # Use 4 encoder stages\n",
    "    decoder_channels=[256, 128, 64,32],  # Decoder channels (adjusted for depth=4)\n",
    "    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=3,                        # Predict 3 classes\n",
    ")\n",
    "\n",
    "loss_function=L.DiceLoss(mode='multilabel')#binary vagy multiclass\n",
    "lr=1e-3\n",
    "optimizer=optim.Adam(model.parameters(), lr=lr, weight_decay=0.000001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.3, patience=3, verbose=True)\n",
    "model_name='./MAnet_Res34_Imgnet_Dice_all_mask.pt'\n",
    "model.load_state_dict(torch.load(model_name, weights_only=True, map_location=torch.device('cuda')))\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6800a05a-50c9-4b0a-9fa2-a832b6c08196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "model.eval()\n",
    "i=0\n",
    "for images, labels in test_dataloader:\n",
    "    images=images.unsqueeze(1)\n",
    "    images=images.to(device=device, dtype=torch.float) \n",
    "    outputs = model(images)\n",
    "    outputs=outputs.squeeze(0)\n",
    "    print(outputs.shape)\n",
    "    #save_image(outputs, 'o.png') save full batch\n",
    "    cv2.imwrite(f'output/manet/{i}.png', outputs.cpu().detach().numpy().T)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b63272-d5a9-4a3d-8a2f-9094ae1e9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "\n",
    "model = smp.DeepLabV3(\n",
    "    encoder_name=\"resnet34\",           # Encoder backbone\n",
    "    encoder_depth=4,                  # Use 4 encoder stages\n",
    "    encoder_weights=\"imagenet\",       # Pre-trained on ImageNet\n",
    "    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=3,                        # Predict 3 classes\n",
    ")\n",
    "\n",
    "loss_function=L.DiceLoss(mode='multilabel')#binary vagy multiclass\n",
    "lr=1e-3\n",
    "optimizer=optim.Adam(model.parameters(), lr=lr, weight_decay=0.000001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.3, patience=3, verbose=True)\n",
    "model_name='./deeplabv3_Res34_Imgnet_Dice_all_mask.pt'\n",
    "model.load_state_dict(torch.load(model_name, weights_only=True, map_location=torch.device('cuda')))\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead74d1b-84a4-46cd-a164-4f7f70d5efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "model.eval()\n",
    "i=0\n",
    "for images, labels in test_dataloader:\n",
    "    images=images.unsqueeze(1)\n",
    "    images=images.to(device=device, dtype=torch.float) \n",
    "    outputs = model(images)\n",
    "    outputs=outputs.squeeze(0)\n",
    "    print(outputs.shape)\n",
    "    #save_image(outputs, 'o.png') save full batch    \n",
    "    cv2.imwrite(f'output/deeplab/{i}.png', outputs.cpu().detach().numpy().T)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09615f9-ff5c-4341-8b83-ce627e6fffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = smp.Linknet(\n",
    "    encoder_name=\"resnet34\",           # Encoder backbone\n",
    "    encoder_depth=4,                  # Use 4 encoder stages\n",
    "    encoder_weights=\"imagenet\",       # Pre-trained on ImageNet\n",
    "    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=3,                        # Predict 3 classes\n",
    ")\n",
    "\n",
    "loss_function=L.DiceLoss(mode='multilabel')#binary vagy multiclass\n",
    "lr=1e-3\n",
    "optimizer=optim.Adam(model.parameters(), lr=lr, weight_decay=0.000001)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.3, patience=3, verbose=True)\n",
    "scheduler = CyclicLR(optimizer, \n",
    "                 base_lr = 0.00006, # Initial learning rate which is the lower boundary in the cycle for each parameter group\n",
    "                 max_lr = 1e-3, # Upper learning rate boundaries in the cycle for each parameter group\n",
    "                 step_size_up = 10, # Number of training iterations in the increasing half of a cycle\n",
    "                 mode = \"exp_range\")\n",
    "model_name='./Linknet_Res34_Imgnet_Dice_all_mask.pt'\n",
    "model.load_state_dict(torch.load(model_name, weights_only=True, map_location=torch.device('cuda')))\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243937c-8337-46f1-9141-d321fbef35d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "model.eval()\n",
    "i=0\n",
    "for images, labels in test_dataloader:\n",
    "    images=images.unsqueeze(1)\n",
    "    images=images.to(device=device, dtype=torch.float) \n",
    "    labels=labels.to(device=device, dtype=torch.float) \n",
    "    outputs = model(images)\n",
    "    labels=labels.squeeze(0)\n",
    "    outputs=outputs.squeeze(0)\n",
    "    print(outputs)\n",
    "    #save_image(outputs, 'o.png') save full batch\n",
    "    #plt.imshow(outputs.cpu().detach().numpy().T)\n",
    "    #plt.show()\n",
    "    #print(outputs.cpu().detach().numpy().T.max())\n",
    "    cv2.imwrite(f'output/linknet/{i}.png', outputs.cpu().detach().numpy().T)\n",
    "    #cv2.imwrite(f'output/orig/{i}.png', labels.cpu().detach().numpy().T)\n",
    "    i+=1\n",
    "    if i>1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8150855-80c3-4324-975f-1fae197bceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(labels.cpu().detach().numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5a20a8-2a15-47cd-8e93-2011f8b3817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(outputs.cpu().detach().numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84292eb0-8cc7-4969-bca6-ec0f509d29ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_trf_val(img_path,mask_path1,mask_path2,mask_path3):\n",
    "\n",
    "    img=cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "    msk1=cv2.imread(str(mask_path1), cv2.IMREAD_GRAYSCALE)\n",
    "    msk2=cv2.imread(str(mask_path2), cv2.IMREAD_GRAYSCALE)\n",
    "    msk3=cv2.imread(str(mask_path3), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    #edged = cv2.Canny(msk, 50, 200) #for edge\n",
    "    #msk=(edged > 0).astype(int) #for edge\n",
    "    max_side1=max(img.shape[0],img.shape[1])\n",
    "    max_side2=max(msk1.shape[0],msk1.shape[1])\n",
    "    max_side=max(max_side1,max_side2)\n",
    "    aug = A.Compose([\n",
    "        A.PadIfNeeded(min_height=max_side, min_width=max_side,border_mode=cv2.BORDER_CONSTANT,value=0,p=1),\n",
    "        A.Resize(128,128),\n",
    "    ],is_check_shapes=False)\n",
    "    \n",
    "    masks=np.stack([msk1,msk2,msk3],axis=0).T\n",
    "\n",
    "    augmented = aug(image=img, mask=masks)\n",
    "    \n",
    "    image_padded = torch.tensor(augmented['image'].T)\n",
    "    mask_padded = torch.tensor(augmented['mask'].T)\n",
    "    return image_padded, mask_padded\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0479fab-bf2a-4293-98f5-dc6710c7e2b4",
   "metadata": {},
   "source": [
    "df=pd.read_csv('test_data_img.csv')\n",
    "\n",
    "df\n",
    "for i in range(len(df)):\n",
    "    a=df['test_mask1'][i]\n",
    "    b=df['test_mask2'][i]\n",
    "    c=df['test_mask3'][i]\n",
    "    ff=img_trf_train(a,a,b,c)\n",
    "    #plt.imshow(ff[1].T)\n",
    "    cv2.imwrite(f'output/orig/{i}.png', ff[1].numpy().T)\n",
    "\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a56e1-a597-40f1-87fd-f220b0294ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
